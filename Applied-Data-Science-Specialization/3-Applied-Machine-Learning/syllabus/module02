Module 2: Supervised Machine Learning - Part 1

This module delves into a wider variety of supervised learning methods for both classification and regression, learning about the connection between model complexity and generalization performance, the importance of proper feature scaling, and how to control model complexity by applying techniques like regularization to avoid overfitting. In addition to k-nearest neighbors, this week covers linear regression (least-squares, ridge, lasso, and polynomial regression), logistic regression, support vector machines, the use of cross-validation for model evaluation, and decision trees.


12 videos, 2 readings

1. Notebook: Module 2 Notebook
2. Video: Introduction to Supervised Machine Learning
3. Video: Overfitting and Underfitting
4. Video: Supervised Learning: Datasets
5. Video: K-Nearest Neighbors: Classification and Regression
6. Video: Linear Regression: Least-Squares
7. Video: Linear Regression: Ridge, Lasso, and Polynomial Regression
8. Video: Logistic Regression
9. Video: Linear Classifiers: Support Vector Machines
10. Video: Multi-Class Classification
11. Video: Kernelized Support Vector Machines
12. Video: Cross-Validation
13. Video: Decision Trees
14. Reading: A Few Useful Things to Know about Machine Learning
15. Reading: Ed Yong: Genetic Test for Autism Refuted (optional)
16. Notebook: Classifier Visualization Playspace
17. Notebook: Assignment 2

Graded: Module 2 Quiz
Graded: Assignment 2 Submission



[READING]
This article by Prof. Pedro Domingos provides a bit more background and discussion of the essential concepts in machine learning covered in Modules 1 and 2. It covers topics such as overfitting, the role of data vs model vs features, and the use of ensembles, where many models are learned instead of just one (something we look at with random forests).

Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 78. doi:10.1145/2347736.2347755



[READING]
This article by Ed Yong in The Scientist is included because it describes a real-world example of a prediction problem in the health/medical sciences domain - training a classifier to predict risk of autism spectrum disorder (ASD) based on genetic markers - as well as including discussion of potential overfitting of the classifier (by training and testing on the same data) as one possible issue, among other factors, by researchers attempting to replicate the study.

http://www.the-scientist.com/?articles.view/articleNo/38030/title/Genetic-Test-for-Autism-Refuted/
