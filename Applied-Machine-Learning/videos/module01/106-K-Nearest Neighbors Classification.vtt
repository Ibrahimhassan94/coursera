WEBVTT

1
00:00:09.169 --> 00:00:13.524
Now that we've gotten a sense for what's
in our data set, as a simple example to

2
00:00:13.524 --> 00:00:17.619
get started, we're going to use this
data set to train a classifier that will

3
00:00:17.619 --> 00:00:21.799
automatically identify any future pieces
of fruit that might come our way.

4
00:00:21.799 --> 00:00:27.191
Based on the features available to
the classifier such as the object's color,

5
00:00:27.191 --> 00:00:28.245
size and mass.

6
00:00:28.245 --> 00:00:33.038
To do this, we'll use a popular and
easy to understand type of machine

7
00:00:33.038 --> 00:00:37.281
learning algorithm known as
k-nearest neighbors or k-NN.

8
00:00:37.281 --> 00:00:42.258
The K-Nearest Neighbors algorithm can be
used for classification and regression.

9
00:00:42.258 --> 00:00:45.988
Though, here we'll focus for the time
being on using it for classification.

10
00:00:45.988 --> 00:00:51.324
k-NN classifiers are an example of
what's called instance based or

11
00:00:51.324 --> 00:00:55.030
memory based supervised learning.

12
00:00:55.030 --> 00:00:59.203
What this means is that instance based
learning methods work by memorizing

13
00:00:59.203 --> 00:01:02.324
the labeled examples that
they see in the training set.

14
00:01:02.324 --> 00:01:06.420
And then they use those memorized
examples to classify new objects later.

15
00:01:08.250 --> 00:01:13.160
The k in k-NN refers to the number of
nearest neighbors the classifier will

16
00:01:13.160 --> 00:01:15.890
retrieve and
use in order to make its prediction.

17
00:01:17.420 --> 00:01:21.450
In particular, the k-NN algorithm has
three steps that can be specified.

18
00:01:21.450 --> 00:01:26.420
First of all, when given a new previously
unseen instance of something to classify,

19
00:01:27.690 --> 00:01:31.880
a k-NN classifier will look into its set
of memorized training examples to find

20
00:01:31.880 --> 00:01:34.780
the k examples that have closest features.

21
00:01:34.780 --> 00:01:37.240
And we'll talk about what
closest means in a minute.

22
00:01:38.750 --> 00:01:41.790
Second, the classifier will
look up the class labels for

23
00:01:41.790 --> 00:01:44.950
those k-Nearest Neighbor examples.

24
00:01:44.950 --> 00:01:48.060
And then once it's done that,
it will combine the labels of those

25
00:01:48.060 --> 00:01:52.420
examples to make a prediction for
the label of the new object.

26
00:01:52.420 --> 00:01:55.900
Typically, for example,
by using simple majority vote.

27
00:01:57.460 --> 00:02:01.190
Let's look at a visual
example of a k-NN classifier

28
00:02:01.190 --> 00:02:03.240
that is based on our fruit data set.

29
00:02:04.348 --> 00:02:08.160
So what I've done here is
taken our training set and

30
00:02:08.160 --> 00:02:12.340
plotted each piece of fruit
using two of its features.

31
00:02:12.340 --> 00:02:13.680
The width and the height.

32
00:02:15.530 --> 00:02:19.230
These two features together form
what is called the feature space

33
00:02:19.230 --> 00:02:20.070
of the classifier.

34
00:02:21.260 --> 00:02:25.720
And I've shown each data point's label
using the colors shown in the legend.

35
00:02:25.720 --> 00:02:29.050
So as you can see, there are four
different colors here that correspond to

36
00:02:29.050 --> 00:02:31.389
the four types of fruit
that are in our data set.

37
00:02:33.240 --> 00:02:38.430
These broad areas of color show
how any point, given a width and

38
00:02:38.430 --> 00:02:42.610
height, could be classified according
to a one nearest neighbor rule.

39
00:02:42.610 --> 00:02:44.958
So, in other words, I've set k = 1 here.

40
00:02:44.958 --> 00:02:51.361
So, for example, a data point over here
that had a height of six centimeters and

41
00:02:51.361 --> 00:02:56.247
a width of nine centimeters
would be classified as an apple.

42
00:02:56.247 --> 00:03:01.379
Because it falls within this
red area here that the k-NN

43
00:03:01.379 --> 00:03:08.817
classifier has decided map two objects
that are apples based on the dimensions.

44
00:03:08.817 --> 00:03:13.075
Or for example if I saw that the
classifier saw a piece of fruit that had

45
00:03:13.075 --> 00:03:16.967
a height of four centimeters and
a width of seven centimeters,

46
00:03:16.967 --> 00:03:19.771
it would be classified
as a mandarin orange.

47
00:03:19.771 --> 00:03:25.400
Because it falls within this decision
area that's assigned to mandarin oranges.

48
00:03:27.890 --> 00:03:30.355
So what does it mean to have
a one nearest neighbor rule?

49
00:03:30.355 --> 00:03:33.790
[COUGH] Where did it come
up with these different

50
00:03:35.710 --> 00:03:38.330
class assignments in the first place?

51
00:03:40.260 --> 00:03:44.710
Well, let's take a look at
the apple example over here.

52
00:03:47.180 --> 00:03:49.960
The object that has a height
of six centimeters and

53
00:03:49.960 --> 00:03:54.950
a width of nine centimeters, if you give
the classifier that point, what it will

54
00:03:54.950 --> 00:03:58.488
do is search within all the different
training examples that it's seen.

55
00:03:58.488 --> 00:04:03.142
So it would look at all these points and
see which one is closest, so

56
00:04:03.142 --> 00:04:06.566
we're looking at the nearest
single neighbor.

57
00:04:06.566 --> 00:04:08.154
So we're looking at k=1.

58
00:04:08.154 --> 00:04:11.076
We're trying to find the one
point that's closest to our,

59
00:04:11.076 --> 00:04:12.770
what's called the query point.

60
00:04:12.770 --> 00:04:14.300
This is the point we want to classify.

61
00:04:15.840 --> 00:04:19.780
So, in this case, we can see probably
that this point over here is the nearest

62
00:04:19.780 --> 00:04:23.490
neighbor of the query
point in feature space.

63
00:04:23.490 --> 00:04:26.280
Similarly, if we look at
the mandarin example over here,

64
00:04:28.220 --> 00:04:33.120
the closest training set object

65
00:04:33.120 --> 00:04:38.110
to the query point is one of these
probably, maybe this point right here.

66
00:04:38.110 --> 00:04:41.120
Okay, so that's the nearest
neighbor of that query point.

67
00:04:42.790 --> 00:04:47.680
So for any query point in
the future space, let's say

68
00:04:47.680 --> 00:04:52.690
we're over here in the lemon zone.

69
00:04:52.690 --> 00:04:56.470
The nearest neighbor to this query
point would be this training set

70
00:04:56.470 --> 00:04:57.180
point right here.

71
00:04:59.000 --> 00:05:02.070
And because the training set points
are labeled, we have the labels for

72
00:05:02.070 --> 00:05:07.450
those, in the one nearest neighbor case,
the classifier

73
00:05:07.450 --> 00:05:12.040
will simply assign to the query point,
the class of the nearest neighbor object.

74
00:05:13.310 --> 00:05:16.500
Because the nearest
neighbor here is an apple,

75
00:05:17.880 --> 00:05:20.970
the query point here will
get classified as an apple.

76
00:05:20.970 --> 00:05:23.690
If we look at the mandarin
orange point over here,

77
00:05:23.690 --> 00:05:26.560
the nearest data point
is the mandarin orange.

78
00:05:26.560 --> 00:05:28.950
So it will get classified in that region.

79
00:05:28.950 --> 00:05:35.276
And if we do that for
every possible query point,

80
00:05:35.276 --> 00:05:40.227
we get this pattern of class regions.

81
00:05:40.227 --> 00:05:44.355
The zones where we transition
from one class to another,

82
00:05:44.355 --> 00:05:48.312
this line right here is
called the decision boundary.

83
00:05:48.312 --> 00:05:52.808
Because query points that are on one side
of the line get mapped to one class.

84
00:05:52.808 --> 00:05:56.620
And points that are on the other side of
the line get mapped to a different class.

85
00:05:58.240 --> 00:06:02.683
So, because this is a k-nearest
neighbor classifier, and

86
00:06:02.683 --> 00:06:05.556
we are looking at the case where k = 1,

87
00:06:05.556 --> 00:06:10.712
we can see that the class boundaries here,
the decision boundaries.

88
00:06:10.712 --> 00:06:12.366
Let's take an example over here.

89
00:06:12.366 --> 00:06:17.464
We have a point over here that's an
orange, another point that's a lemon here.

90
00:06:17.464 --> 00:06:21.860
And we can see the decision boundary
because it's based on Euclidean

91
00:06:21.860 --> 00:06:26.562
distance and we're weighting points
equally that the decision boundary

92
00:06:26.562 --> 00:06:30.475
goes exactly at a distance
equidistant to these two things.

93
00:06:30.475 --> 00:06:32.660
That's exactly halfway between
these two points, right?

94
00:06:32.660 --> 00:06:36.080
Because if it were any
closer to this point,

95
00:06:36.080 --> 00:06:38.740
it would be mapped to the orange class.

96
00:06:38.740 --> 00:06:41.862
And if it were any closer to
the lemon instance over here,

97
00:06:41.862 --> 00:06:43.760
it would be mapped as lemon class.

98
00:06:43.760 --> 00:06:50.460
So you can see this pattern for
all the decision boundaries here.

99
00:06:50.460 --> 00:06:54.990
This line is equidistant to
these two nearest points.

100
00:06:58.940 --> 00:07:03.557
So this is basically how the K-Nearest
Neighbor classifies new instances.

101
00:07:03.557 --> 00:07:10.634
So you can imagine with query
points where k is larger than 1,

102
00:07:10.634 --> 00:07:17.179
we might use a slightly more
complicated decision rule.

103
00:07:17.179 --> 00:07:20.189
Let's take this example over here.

104
00:07:20.189 --> 00:07:22.516
Suppose we have a query
point that's in orange.

105
00:07:22.516 --> 00:07:24.967
And we're doing two nearest
neighbours classifications.

106
00:07:24.967 --> 00:07:31.090
So in that case, the two nearest
points are these two points here.

107
00:07:31.090 --> 00:07:36.170
And in cases where k is bigger than 1,
we use a simple majority vote.

108
00:07:36.170 --> 00:07:42.105
We take the class that's most predominant
in the labels of the neighbor examples.

109
00:07:42.105 --> 00:07:45.011
So in this case, the labels happen
to agree they're both oranges.

110
00:07:45.011 --> 00:07:49.943
And so this point in a two nearest
neighbors situation would be classified as

111
00:07:49.943 --> 00:07:50.720
an orange.

112
00:07:51.890 --> 00:07:57.060
When you get points that are closer to,
say, a point

113
00:07:57.060 --> 00:08:02.110
over here, the two nearest neighbors,
in that case, might be these points here.

114
00:08:03.810 --> 00:08:07.570
In which case you could choose randomly
between them to break the tie.

115
00:08:07.570 --> 00:08:10.980
Typically, the value of k is odd, so

116
00:08:10.980 --> 00:08:15.930
that we can simply,
k equals three it might look like this.

117
00:08:15.930 --> 00:08:20.258
And so, you can see that the three nearest
neighbors here are a red point an apple,

118
00:08:20.258 --> 00:08:23.230
a blue point an orange, and
another red point an apple.

119
00:08:23.230 --> 00:08:29.570
So, in the k = 3 case, the vote would
go towards labeling this as an apple.

120
00:08:31.490 --> 00:08:39.120
And that's basically all there is to
the basic mechanism for k-NN classifiers.

121
00:08:39.120 --> 00:08:41.390
More generally,

122
00:08:41.390 --> 00:08:45.830
to use the nearest neighbor algorithm,
we specify four things.

123
00:08:45.830 --> 00:08:50.080
First, we need to define what distance
means in our future space, so

124
00:08:50.080 --> 00:08:54.180
we know how to properly
select the nearby neighbors.

125
00:08:54.180 --> 00:08:59.310
In the example that I just showed you with
the fruit data set, we used the simple

126
00:08:59.310 --> 00:09:04.669
straight line, or euclidean distance
to measure the distance between points.

127
00:09:06.130 --> 00:09:09.940
Second, we must tell the algorithm how
many of these nearest neighbors to use in

128
00:09:09.940 --> 00:09:11.740
making a prediction.

129
00:09:11.740 --> 00:09:13.590
This must be a number
that is at least one.

130
00:09:15.150 --> 00:09:18.560
Third, we may want to give some
neighbors more influence on the outcome.

131
00:09:18.560 --> 00:09:23.400
For example, we may decide
that neighbors that are closer

132
00:09:23.400 --> 00:09:27.530
to the new instance that we're trying to
classify, should have more influence, or

133
00:09:27.530 --> 00:09:29.530
more votes on the final label.

134
00:09:30.970 --> 00:09:33.450
Finally, once we have the labels
of the k nearby points,

135
00:09:33.450 --> 00:09:36.870
we must specify how to combine them
to produce a final prediction.

136
00:09:38.990 --> 00:09:42.180
Here's a specific example of some
typical choices we might make for

137
00:09:42.180 --> 00:09:43.159
these four things.

138
00:09:44.310 --> 00:09:48.670
The most common distance metric and
the one that scikit-learn uses by default

139
00:09:48.670 --> 00:09:50.900
is the euclidean no
straight line distance.

140
00:09:52.100 --> 00:09:56.977
So, technically if you are interested,
the euclidean metric is actually a special

141
00:09:56.977 --> 00:10:00.755
case of a more general metric
called the Minkowski metric, where

142
00:10:00.755 --> 00:10:05.448
there is a parameter p that's set to two
that will give you the euclidean metric.

143
00:10:05.448 --> 00:10:09.496
And you'll see this in the notebook,
when you look at the actual settings,

144
00:10:09.496 --> 00:10:11.910
there done in terms of
the Minkowski metric.

145
00:10:13.460 --> 00:10:18.180
We might choose to use the five nearest
neighbors, let's say, for our choice of k.

146
00:10:19.270 --> 00:10:22.830
And, we might specify that
there's no special treatment for

147
00:10:22.830 --> 00:10:26.170
neighbors that are closer, so
we have a uniform waiting.

148
00:10:27.800 --> 00:10:30.670
Also by default
scikit-learn will apply for

149
00:10:30.670 --> 00:10:35.860
the fourth criterion a simple
majority vote, and it will predict

150
00:10:35.860 --> 00:10:39.190
the class with the most representatives
among the nearest neighbors.

151
00:10:40.300 --> 00:10:44.050
We'll look at a number of alternative
classifiers in next week's class on

152
00:10:44.050 --> 00:10:45.539
supervised learning methods.

153
00:10:46.720 --> 00:10:51.100
For now, let's take a look at the notebook
to see how we apply a k a nearest neighbor

154
00:10:51.100 --> 00:10:54.620
classifier in Python to our
example fruit data set.

155
00:10:57.090 --> 00:11:01.830
As a reminder you can use Shit+tab to show
the documentation pop up for the method as

156
00:11:01.830 --> 00:11:06.019
you're entering it to help remind you of
the different parameter option and syntax.

157
00:11:07.950 --> 00:11:12.440
So, recall that our task was to correctly
classify new, incoming objects.

158
00:11:12.440 --> 00:11:15.380
In our example case, pieces of fruit.

159
00:11:15.380 --> 00:11:19.950
So, our newly trained classifier will
take a data instance as input, and

160
00:11:19.950 --> 00:11:22.160
give us a predicted label as output.

161
00:11:23.520 --> 00:11:29.500
The first step in Python that we need to
take is to load the necessary modules.

162
00:11:29.500 --> 00:11:32.530
As well as load the data set
into a panda's data frame.

163
00:11:35.010 --> 00:11:39.640
After we've done that, we can dump out
the first few rows using the head method

164
00:11:39.640 --> 00:11:43.590
to check the column names, and get some
examples of the first few instances.

165
00:11:46.040 --> 00:11:50.110
Now, what I'm doing next here in
the notebook is defining a dictionary

166
00:11:50.110 --> 00:11:53.610
that takes a numeric fruit
label as the input key.

167
00:11:53.610 --> 00:11:57.510
And returns a value that's a string with
the name of the fruit, and this dictionary

168
00:11:57.510 --> 00:12:02.450
just makes it easier to convert the output
of a classifier prediction to something

169
00:12:02.450 --> 00:12:06.430
a person can more easily interpret,
the name of a fruit in this case.

170
00:12:08.010 --> 00:12:11.570
So, for this example we'll
define a variable capital X

171
00:12:11.570 --> 00:12:15.200
that holds the features of our
data set without the label.

172
00:12:16.330 --> 00:12:21.300
And here I 'm going to use the mass,
width and height of the fruit as features.

173
00:12:22.340 --> 00:12:24.780
So, this collection of features
is called the feature space.

174
00:12:26.370 --> 00:12:28.830
We define a second variable, lower case y,

175
00:12:28.830 --> 00:12:32.949
to hold the corresponding labels for
the instances in x.

176
00:12:34.340 --> 00:12:39.660
Now, we can parse x and y to the train
test split function in circuit LAN.

177
00:12:41.260 --> 00:12:44.744
Normally these splitting into training and
test sets is done randomly, but for

178
00:12:44.744 --> 00:12:47.297
this lecture I want to make sure
we all get the same results.

179
00:12:47.297 --> 00:12:50.677
So, I set the random state
parameter to a specific f=value,

180
00:12:50.677 --> 00:12:52.700
in this case I happen to choose zero.

181
00:12:54.830 --> 00:12:57.560
The results of the train
test split function

182
00:12:57.560 --> 00:13:01.560
are put into the four
variables you see on the left.

183
00:13:01.560 --> 00:13:07.790
And these are marked as x_train,
x_test, y_train, and y_test.

184
00:13:07.790 --> 00:13:11.040
We're going to be using this x and
y variable naming convention throughout

185
00:13:11.040 --> 00:13:15.420
the course to refer to data and labels.

186
00:13:17.690 --> 00:13:19.800
Once we have our train-test split,

187
00:13:19.800 --> 00:13:24.360
we then need to create an instance
of the classifier object.

188
00:13:24.360 --> 00:13:26.210
And in this case a k-NN classifier.

189
00:13:28.060 --> 00:13:32.380
And the set the important parameter in
this case the number of neighbors to

190
00:13:32.380 --> 00:13:35.570
a specific value to be
used by the classifier.

191
00:13:38.510 --> 00:13:44.290
We then train the classifier by passing
in the training set date in X_train,

192
00:13:44.290 --> 00:13:48.580
and the labels in y_train to
the classifiers fit method.

193
00:13:49.858 --> 00:13:54.160
Now, the k-NN classifier that I'm
using in this case is an example of

194
00:13:54.160 --> 00:13:58.730
a more general class called
an estimator in scikit-learn.

195
00:13:58.730 --> 00:14:02.740
So, all estimators have a fit method
that takes the training data, and

196
00:14:02.740 --> 00:14:07.400
then changes the state of the classifier,
or estimator object

197
00:14:08.990 --> 00:14:13.539
to essentially enable prediction
once the training is finished.

198
00:14:14.870 --> 00:14:19.170
In other words it updates the state of
the k and n variable here, which means,

199
00:14:19.170 --> 00:14:23.500
that in the case of k-nners neighbors it
will memorize the training set examples

200
00:14:23.500 --> 00:14:26.480
in some kind of internal storage for
future use.

201
00:14:29.160 --> 00:14:32.610
And that's really all there is to
training the k-NN classifier, and

202
00:14:32.610 --> 00:14:37.310
the first thing we can do with this
newly trained classifier is to see

203
00:14:37.310 --> 00:14:41.960
how accurate it's likely to be on some
new, previously unseen instances.

204
00:14:42.970 --> 00:14:48.010
To do this, we can apply they classifier
to all the instanced in the test set

205
00:14:48.010 --> 00:14:49.810
that we've put aside.

206
00:14:49.810 --> 00:14:52.350
Since these test instances were
not explicitly included in

207
00:14:52.350 --> 00:14:53.550
the classifiers training.

208
00:14:55.220 --> 00:14:58.950
One simple way to assess if the classifier
is likely to be good at predicting

209
00:14:58.950 --> 00:15:04.150
the label of future,
previously unseen data instances,

210
00:15:04.150 --> 00:15:08.540
is to compute the classifier's
accuracy on the test set data items.

211
00:15:09.980 --> 00:15:13.830
Remember, that the k-NN classifier
did not see any of the fruits

212
00:15:13.830 --> 00:15:16.029
in the test set during the training phase.

213
00:15:17.500 --> 00:15:21.070
To do this we use the score method for
the classifier object.

214
00:15:22.070 --> 00:15:26.290
This will take the test set points
as input and compute the accuracy.

215
00:15:26.290 --> 00:15:30.910
The accuracy is defined as
the fraction of test set items,

216
00:15:30.910 --> 00:15:34.280
whose true label was correctly
predicted by the classifier.

217
00:15:37.410 --> 00:15:41.980
We can also use our new classifier to
classify individual instances of a fruit.

218
00:15:41.980 --> 00:15:43.950
In fact this was our
goal in the first place.

219
00:15:43.950 --> 00:15:47.720
Was to be able to take individual
instances of objects and

220
00:15:47.720 --> 00:15:48.600
assign them a label.

221
00:15:49.760 --> 00:15:51.310
So, here, for example.

222
00:15:51.310 --> 00:15:53.630
I'll enter the mass, width and height for

223
00:15:53.630 --> 00:15:57.660
a hypothetical piece of
fruit that is fairly small.

224
00:15:57.660 --> 00:16:03.370
And if we ask the classifier to predict
the label using the predict method.

225
00:16:04.540 --> 00:16:07.100
We can see the output is that it
predicts it's a mandarin orange.

226
00:16:08.750 --> 00:16:13.040
I can then pass a different example, which
is maybe a larger, slightly elongated

227
00:16:13.040 --> 00:16:17.130
fruit that has a height that's greater
than the width and a larger mass.

228
00:16:17.130 --> 00:16:21.280
In this case,
using the predict method on this instance

229
00:16:21.280 --> 00:16:25.680
results in a label that says the
classifier thinks this object is a lemon.

230
00:16:28.230 --> 00:16:32.770
Now let's use a utility function called
plot fruit knn that's included in

231
00:16:32.770 --> 00:16:35.510
the shared utilities module
that comes with this course.

232
00:16:36.750 --> 00:16:39.950
This will produce the colored plots
I showed you earlier that have

233
00:16:39.950 --> 00:16:41.100
the decision boundaries.

234
00:16:42.830 --> 00:16:45.040
You can then try out
different values of k for

235
00:16:45.040 --> 00:16:49.030
yourself to see what the effect
is on the decision boundaries.

236
00:16:50.660 --> 00:16:54.070
The uniformed parameter
that I pass in here,

237
00:16:54.070 --> 00:16:57.740
as the last parameter is
the waiting method to be used.

238
00:16:57.740 --> 00:17:00.500
So here I'm passing in the string uniform,

239
00:17:00.500 --> 00:17:05.810
which means to treat all neighbours
equally when combining their labels.

240
00:17:05.810 --> 00:17:08.569
If you like, you can try changing
this to the word distance,

241
00:17:10.030 --> 00:17:11.890
if you want to try a distance wave method.

242
00:17:13.250 --> 00:17:16.200
You can also pass your own function,
but we'll leave that for later.

243
00:17:18.890 --> 00:17:22.370
We can also see how our new classifier
behaves for different values of k.

244
00:17:23.800 --> 00:17:26.840
So in this series of plots,

245
00:17:26.840 --> 00:17:31.867
we can see the different
decision boundaries

246
00:17:31.867 --> 00:17:37.960
that are produced as k is
varied from one to five to ten.

247
00:17:37.960 --> 00:17:43.520
We can see that when K has a small
value like 1, the classifier

248
00:17:43.520 --> 00:17:47.540
is good at learning the classes for
individual points in the training set.

249
00:17:47.540 --> 00:17:51.009
But with a decision boundary that's
fragmented with considerable variation.

250
00:17:52.360 --> 00:17:58.460
This is because when K = 1, the prediction
is sensitive to noise, outliers,

251
00:17:58.460 --> 00:18:02.470
mislabeled data, and other sources of
variation in individual data points.

252
00:18:03.650 --> 00:18:08.710
For larger values of K, the areas assigned
to different classes are smoother and

253
00:18:08.710 --> 00:18:13.670
not as fragmented and more robust
to noise in the individual points.

254
00:18:13.670 --> 00:18:18.637
But possibly with some mistakes,
more mistakes in individual points.

255
00:18:18.637 --> 00:18:22.021
This is an example of what's known
as the bias variance tradeoff.

256
00:18:22.021 --> 00:18:24.180
And we'll look at that phenomenon and

257
00:18:24.180 --> 00:18:27.290
its implications in more
depth in next week's class.

258
00:18:29.500 --> 00:18:32.690
Given the changes in the classifier's
decision boundaries we observed when we

259
00:18:32.690 --> 00:18:36.830
changed k, the natural question
might be how the value of k,

260
00:18:36.830 --> 00:18:40.310
how the choice of k,
affects the accuracy of the classifier.

261
00:18:41.710 --> 00:18:45.508
We can plot the accuracy as a function
of k very easily using this short

262
00:18:45.508 --> 00:18:46.500
snippet of code.

263
00:18:49.963 --> 00:18:54.546
We see that, indeed, larger values
of k do lead to worse accuracy for

264
00:18:54.546 --> 00:18:58.590
this particular dataset and
fixed single train test split.

265
00:18:59.600 --> 00:19:02.240
Keep in mind though,
these results are only for

266
00:19:02.240 --> 00:19:03.960
this particular training test split.

267
00:19:04.970 --> 00:19:09.150
To get a more reliable estimate of likely
future accuracy for a particular value of

268
00:19:09.150 --> 00:19:14.980
k, we would want to look at results over
multiple possible train test splits.

269
00:19:14.980 --> 00:19:19.090
We'll go into this issue of model
selection next week as well.

270
00:19:21.160 --> 00:19:25.000
In general, the best choice of the value
of k, that is the one that leads to

271
00:19:25.000 --> 00:19:28.270
the highest accuracy, can vary
greatly depending on the data set.

272
00:19:29.400 --> 00:19:31.578
In general with k-nearest neighbors,

273
00:19:31.578 --> 00:19:35.550
using a larger k suppresses the effects
of noisy individual labels.

274
00:19:35.550 --> 00:19:38.530
But results in classification
boundaries that are less detailed.

275
00:19:41.132 --> 00:19:47.780
So we've taken several steps
in this course so far.

276
00:19:47.780 --> 00:19:51.930
We've looked at a data set,
plotted some features.

277
00:19:51.930 --> 00:19:57.035
We then took the features and
learned how to compute a train test split.

278
00:19:57.035 --> 00:19:59.265
Used that to train a classifier and

279
00:19:59.265 --> 00:20:03.240
used the resulting classifier to make
some predictions for new objects.

280
00:20:03.240 --> 00:20:05.100
So congratulations,
you've just created and

281
00:20:05.100 --> 00:20:07.540
run your first machine learning
application in Python.

282
00:20:08.930 --> 00:20:13.110
In next week's class, we'll go into some
supervised learning methods in more depth.

283
00:20:13.110 --> 00:20:15.680
And look beyond k-nearest
neighbors to learn how and

284
00:20:15.680 --> 00:20:19.230
why to apply other types of classifiers
to machine learning problems.