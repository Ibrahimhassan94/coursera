WEBVTT

1
00:00:09.120 --> 00:00:11.602
For our first machine
learning exploration,

2
00:00:11.602 --> 00:00:15.810
we're going to build an extremely simple
form of object recognition system.

3
00:00:17.230 --> 00:00:21.670
Now although the example we'll use is very
simple, it does reflect many of the same

4
00:00:21.670 --> 00:00:27.022
key machine learning concepts that go into
building real world commercial systems.

5
00:00:27.022 --> 00:00:31.060
The dataset we're going to use is a small,
very simple,

6
00:00:31.060 --> 00:00:36.227
example dataset derived from
one originally created by Dr.

7
00:00:36.227 --> 00:00:40.960
Iain Murray at the University
of Edinburgh for the task of

8
00:00:40.960 --> 00:00:44.580
training a classifier to distinguish
between different types of fruit.

9
00:00:46.030 --> 00:00:48.080
So to create the original dataset, Dr.

10
00:00:48.080 --> 00:00:52.820
Murray went to a nearby store,
bought a few dozen oranges, lemons, and

11
00:00:52.820 --> 00:00:57.350
apples of different varieties, and
recorded their measurements in a table.

12
00:00:57.350 --> 00:01:01.520
So he sat down and
he looked at the height and

13
00:01:01.520 --> 00:01:05.039
the width, estimated their mass,
and so forth.

14
00:01:06.130 --> 00:01:10.230
So, we've formatted his original
data slightly and added one or

15
00:01:10.230 --> 00:01:16.890
two extra simulated features such as
a color score for instructional purposes.

16
00:01:16.890 --> 00:01:21.060
This dataset is called
fruit_data_with_colors.txt.

17
00:01:21.060 --> 00:01:24.230
And it's included in the folder of
materials that you downloaded for

18
00:01:24.230 --> 00:01:24.879
this course.

19
00:01:26.380 --> 00:01:29.370
Now you might think that fruit
prediction is a sort of silly and

20
00:01:29.370 --> 00:01:30.690
impractical scenario.

21
00:01:30.690 --> 00:01:35.370
And given the limited nature of this
dataset, it is a bit of a toy example.

22
00:01:35.370 --> 00:01:39.900
But actually, food companies do indeed
now rely on machine learning systems

23
00:01:39.900 --> 00:01:44.320
that aren't all that different in concept
from the ones we're about to build, so

24
00:01:44.320 --> 00:01:47.160
they can do automated quality control.

25
00:01:47.160 --> 00:01:50.620
And in fact, there do exist for
example, real systems used by

26
00:01:51.900 --> 00:01:57.610
fruit shipping companies that screen for
rotten oranges during processing.

27
00:01:57.610 --> 00:02:02.490
Now the features that they use in
building these systems are a little more

28
00:02:02.490 --> 00:02:05.000
sophisticated than the ones
we'll look at here.

29
00:02:05.000 --> 00:02:10.050
So for example,
quality control systems for

30
00:02:11.590 --> 00:02:16.490
rotten orange detection use ultraviolet
light that can detect interior decay,

31
00:02:16.490 --> 00:02:20.060
which is often less visible than
just by looking on the surface.

32
00:02:20.060 --> 00:02:23.990
Anyway, to solve machine
learning problems,

33
00:02:23.990 --> 00:02:26.780
you can thing of the input
data as a table.

34
00:02:26.780 --> 00:02:32.250
Where each object, so in our case a piece
of fruit, is represented by a row,

35
00:02:32.250 --> 00:02:37.110
and the attributes of the object, the
measurement, the color, the size, and so

36
00:02:37.110 --> 00:02:41.230
forth in our case for a piece of fruit,
the features of the fruit are represented

37
00:02:41.230 --> 00:02:44.159
by the values that you
see across the columns.

38
00:02:45.550 --> 00:02:50.450
In a supervised learning problem,
the dataset will also typically contain

39
00:02:50.450 --> 00:02:53.750
a special column with
the label of the object,

40
00:02:53.750 --> 00:02:57.390
if the dataset does not
have such a field already.

41
00:02:58.430 --> 00:03:03.629
Sometimes you can derive it from
information that's in one or more columns.

42
00:03:05.380 --> 00:03:12.480
So, to make sure you're ready to continue,
make sure you've run the following code

43
00:03:12.480 --> 00:03:17.980
snippet that loads the proper libraries
we're going to need to proceed.

44
00:03:17.980 --> 00:03:19.319
And we'll show those here now.

45
00:03:41.710 --> 00:03:46.062
So the first thing we are going to
do is to load the fruit dataset

46
00:03:46.062 --> 00:03:50.900
file using the very handy
read table command in pandas.

47
00:03:50.900 --> 00:03:54.980
Now, this will read the dataset from disk,
and

48
00:03:54.980 --> 00:03:59.080
store it into a data frame variable
that we'll call fruits here.

49
00:04:01.240 --> 00:04:06.010
Okay, let's look at this dataset and dump
out the first few rows of the data frame.

50
00:04:08.190 --> 00:04:12.680
So, here we can see that each row of
the dataset represents one piece of fruit

51
00:04:12.680 --> 00:04:16.700
as represented by several features
that are in the table's columns.

52
00:04:16.700 --> 00:04:21.100
So, in order,
the columns that we see are fruit label.

53
00:04:21.100 --> 00:04:23.080
So this is the training
label that we used.

54
00:04:23.080 --> 00:04:26.270
It's a number that corresponds
to the general type of fruit.

55
00:04:26.270 --> 00:04:30.860
So for example, one is an apple,
two is a mandarin orange,

56
00:04:30.860 --> 00:04:32.690
three is a regular orange, and so forth.

57
00:04:34.175 --> 00:04:37.855
So this label was supplied by
the human creator of the dataset.

58
00:04:39.690 --> 00:04:43.050
The fruit name and fruit subtype columns

59
00:04:43.050 --> 00:04:47.329
contain text descriptions of the general
and specific fruit categories.

60
00:04:48.730 --> 00:04:50.450
And the fruit name corresponds,

61
00:04:50.450 --> 00:04:55.090
it's a text form of the corresponding
fruit label in the same row.

62
00:04:56.410 --> 00:04:59.380
Now, we won't be using these
name columns as features.

63
00:04:59.380 --> 00:05:03.230
I've just included them here to make
the dataset a bit more readable for

64
00:05:03.230 --> 00:05:04.069
our purposes.

65
00:05:05.830 --> 00:05:10.640
After that, the features in this
representation include measurements for

66
00:05:10.640 --> 00:05:16.970
each fruit that capture its mass in grams
and its width and height in centimeters.

67
00:05:18.010 --> 00:05:24.100
And finally, there's a feature stored
in a column called color score that's

68
00:05:24.100 --> 00:05:28.400
meant to be a single number that captures
a rough idea of the color of the fruit.

69
00:05:28.400 --> 00:05:32.755
So in a real system, this would actually
be something more sophisticated,

70
00:05:32.755 --> 00:05:35.681
like a histogram of
the distribution of colors, or

71
00:05:35.681 --> 00:05:39.830
maybe the pixels from an actual image or
video of the fruit.

72
00:05:39.830 --> 00:05:44.620
But for our purposes, we're going to
just summarize the color along

73
00:05:44.620 --> 00:05:49.110
a spectrum scale that'll

74
00:05:49.110 --> 00:05:53.670
be a handy summary that we can use
that will just be easy to visualize.

75
00:05:53.670 --> 00:05:57.960
So scores close to one
mean the fruit is red.

76
00:05:57.960 --> 00:06:02.319
Scores around 0.7 indicate yellow,
and so forth.

77
00:06:03.710 --> 00:06:09.840
So, looking at this data frame, we can see
that it contains 59 rows corresponding

78
00:06:09.840 --> 00:06:15.300
to 59 different pieces of fruit that have
been measured and entered into the table.

79
00:06:15.300 --> 00:06:19.130
So our goal here is to build
a classifier from this data

80
00:06:19.130 --> 00:06:23.240
that can predict the correct
type of fruit for any given

81
00:06:25.370 --> 00:06:29.660
observation of its features such as mass,
height, width, and color score.

82
00:06:29.660 --> 00:06:34.810
So for example, can we tell based on
the color score and the dimensions,

83
00:06:34.810 --> 00:06:40.160
the difference between an orange and
a lemon and have the classifier predict

84
00:06:40.160 --> 00:06:44.409
the type of piece of fruit correctly
just from its observed measurements?

85
00:06:46.290 --> 00:06:50.240
Now, assuming for the moment that we
already we had a classifier ready to go,

86
00:06:50.240 --> 00:06:54.320
how would we know if its predictions
were likely to be accurate?

87
00:06:55.480 --> 00:06:58.780
Well, we could choose a fruit sample,
called a test sample, for

88
00:06:58.780 --> 00:07:00.400
which we already had a label.

89
00:07:00.400 --> 00:07:04.990
So we could feed the features of that
piece of fruit into the classifier, and

90
00:07:04.990 --> 00:07:08.750
then compare the label that
the classifier predicts

91
00:07:08.750 --> 00:07:13.370
with the actual true
label of the fruit type.

92
00:07:13.370 --> 00:07:18.450
So, here's a very important point though,
if we use one of our labeled

93
00:07:18.450 --> 00:07:23.640
fruit examples in the data that we use to
train the classifier, we can't also use

94
00:07:23.640 --> 00:07:29.550
that same fruit sample later as a test
sample to also evaluate the classifier.

95
00:07:30.800 --> 00:07:31.520
Why is that?

96
00:07:31.520 --> 00:07:37.210
Well, a key ability that our classifier
needs to have is that it needs to work

97
00:07:37.210 --> 00:07:43.160
well on any input sample, any new pieces
of fruit that we might see in the future,

98
00:07:43.160 --> 00:07:45.769
not just on the ones that we
have on our training set.

99
00:07:46.990 --> 00:07:50.680
Because our classifier could simply
memorize every sample in the training set,

100
00:07:50.680 --> 00:07:55.870
it'd be pretty trivial to just
give back the correct label for

101
00:07:55.870 --> 00:07:58.350
any one of the same samples later, right?

102
00:07:58.350 --> 00:08:03.184
So, measuring the classifier's performance
later using the same samples that

103
00:08:03.184 --> 00:08:07.518
we've used to train it in the first
place doesn't tell us anything about

104
00:08:07.518 --> 00:08:12.447
how well the classifier is likely to work
for a fruit that we haven't seen before.

105
00:08:12.447 --> 00:08:16.320
It will only tell us what we already
know about what's in the training set.

106
00:08:18.180 --> 00:08:22.250
So since our only source of labeled
data is the dataset we've been given,

107
00:08:23.680 --> 00:08:28.260
to estimate how well the classifier
will do on future samples,

108
00:08:29.400 --> 00:08:33.870
what we'll do is split the original
dataset into two parts.

109
00:08:33.870 --> 00:08:37.520
We'll have an array of labeled
samples called the training set

110
00:08:37.520 --> 00:08:40.760
that will be used to train the classifier.

111
00:08:40.760 --> 00:08:43.720
And then we'll hold out
the remaining labeled samples and

112
00:08:43.720 --> 00:08:47.660
put them into a second separate
array called the test set

113
00:08:47.660 --> 00:08:51.119
that will be used to then
evaluate the trained classifier.

114
00:08:52.690 --> 00:08:55.998
So to create training and
test sets from a input dataset,

115
00:08:55.998 --> 00:09:00.780
Scikit-learn provides a handy function
that will do this split for us,

116
00:09:00.780 --> 00:09:04.750
called, not surprisingly,
train test split.

117
00:09:04.750 --> 00:09:06.520
And here's an example of
how we're going to use it.

118
00:09:08.230 --> 00:09:11.043
This function randomly
shuffles the dataset and

119
00:09:11.043 --> 00:09:15.571
splits off a certain percentage of the
input samples for use as a training set,

120
00:09:15.571 --> 00:09:20.390
and then puts the remaining samples into a
different variable for use as a test set.

121
00:09:20.390 --> 00:09:26.630
So in this example, we're using a 75-25%
split of training versus test data.

122
00:09:26.630 --> 00:09:30.536
And that's a pretty standard
relative split that's used.

123
00:09:30.536 --> 00:09:35.491
It's a good rule of thumb to use in
deciding what proportion of training

124
00:09:35.491 --> 00:09:37.650
versus test might be helpful.

125
00:09:39.310 --> 00:09:41.680
As a reminder,
when we're using Scikit-learn,

126
00:09:41.680 --> 00:09:46.955
we'll denote the data that we have using
different flavors of the variable X,

127
00:09:46.955 --> 00:09:51.820
capital X, which is typically a two
dimensional array or data frame.

128
00:09:53.240 --> 00:09:58.850
And the notation we'll use for labels
will be typically based on lowercase y,

129
00:09:58.850 --> 00:10:02.110
which is usually a one dimensional array,
or a scalar.

130
00:10:04.460 --> 00:10:07.779
Now, note the use of
the random state parameter in

131
00:10:07.779 --> 00:10:11.060
the train_test_split function.

132
00:10:11.060 --> 00:10:15.386
So this random state parameter provides
a seed value to the function's internal

133
00:10:15.386 --> 00:10:16.860
random number generator.

134
00:10:18.580 --> 00:10:21.320
If we choose different values for
that seed value,

135
00:10:21.320 --> 00:10:26.206
that will result in different randomized
splits for training and test.

136
00:10:26.206 --> 00:10:29.240
So, if we want to get the same
training and test split each time,

137
00:10:29.240 --> 00:10:33.740
we just make sure to pass in the same
value of the random state parameter.

138
00:10:33.740 --> 00:10:37.770
And so here, we're going to set that
parameter to zero for all our examples.

139
00:10:39.450 --> 00:10:45.410
The training test split function will
put the training set here into X_train,

140
00:10:45.410 --> 00:10:50.340
the test set into X_test,
the training labels into y_train,

141
00:10:50.340 --> 00:10:53.010
and the test labels into y_test.

142
00:10:53.010 --> 00:10:57.799
So this is a 75-25 partitioning of
the original data into these two parts.

143
00:10:59.480 --> 00:11:01.780
And this is the variable naming
convention we'll use for

144
00:11:01.780 --> 00:11:03.340
pretty much all of our coding.

145
00:11:04.780 --> 00:11:06.820
We'll put the rows of
the data without the label,

146
00:11:06.820 --> 00:11:10.540
the training instances into
this capital X variable, and

147
00:11:10.540 --> 00:11:16.160
the list of corresponding labels for those
rows into a variable called lowercase y.

148
00:11:18.160 --> 00:11:22.110
When using the training set and
the test set, we'll then

149
00:11:22.110 --> 00:11:27.060
use X_train that holds the training
instances to train the classifier,

150
00:11:28.080 --> 00:11:32.910
and X_test to evaluate the classifier
after it's been trained.

151
00:11:34.680 --> 00:11:39.180
And we're going to show a short snippet
of code here now that illustrates

152
00:11:39.180 --> 00:11:40.580
how to do that.

153
00:11:40.580 --> 00:11:45.464
So here we can see the results of the
function, applying this train_test_split

154
00:11:45.464 --> 00:11:50.347
function, you can see that it has indeed
split our fruit dataset into training and

155
00:11:50.347 --> 00:11:53.413
test sets with the correct
proportion of samples.

156
00:11:55.738 --> 00:11:59.350
Now that we have a training and
a test set, we're ready for the next step.

157
00:12:01.600 --> 00:12:06.180
And we'll look now into more
depth at the data itself before

158
00:12:06.180 --> 00:12:08.800
we proceed with giving it to
a machine learning algorithm.