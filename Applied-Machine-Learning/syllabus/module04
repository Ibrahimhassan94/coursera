Module 4: Supervised Machine Learning - Part 2

This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees),
and neural networks (with an optional summary on deep learning).

You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.

10 videos, 11 readings

1. Notebook: Module 4 Notebook
2. Video: Naive Bayes Classifiers
3. Video: Random Forests
4. Video: Gradient Boosted Decision Trees
5. Video: Neural Networks
6. Reading: Neural Networks Made Easy (optional)
7. Reading: Play with Neural Networks: TensorFlow Playground (optional)
8. Video: Deep Learning (Optional)
9. Reading: Deep Learning in a Nutshell: Core Concepts (optional)
10. Reading: Assisting Pathologists in Detecting Cancer with Deep Learning (optional)
11. Video: Data Leakage
12. Reading: The Treachery of Leakage (optional)
13. Reading: Leakage in Data Mining: Formulation, Detection, and Avoidance (optional)
14. Reading: Data Leakage Example: The ICML 2013 Whale Challenge (optional)
15. Reading: Rules of Machine Learning: Best Practices for ML Engineering (optional)
16. Notebook: Assignment 4
17. Notebook: Unsupervised Learning Notebook
18. Video: Introduction
19. Video: Dimensionality Reduction and Manifold Learning
20. Video: Clustering
21. Reading: How to Use t-SNE Effectively
22. Reading: How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms
23. Video: Conclusion
24. Reading: Post-course Survey

Graded: Module 4 Quiz
Graded: Assignment 4 Submission



[READING: NEURAL NETWORKS MADE EASY]
This tutorial by Ophir Tanz and Cambron Carter is a fun high-level math-free tutorial on neural networks and in particular, goes into more depth on convolutional neural networks - a form of neural network with multiple layers of processing that forms the basis for many deep learning systems today (see the Deep Learning lecture for more details).

Carter, C., & Tanz, O. (2017, April 13). Neural networks made easy. Retrieved May 10, 2017, from https://techcrunch.com/2017/04/13/neural-networks-made-easy/



[READING: PLAY WITH NEURAL NETWORKS: TensorFlow Playground (optional)]

This neural network simulation by Daniel Smilkov and Shan Carter lets you play with neural networks in your browser. See the effect of different parameter settings and network configurations on a choice of difficult example classification problems.

The "output" on the right shows the "training loss" and "test loss". Loss is an evaluation metric that is related to the number of errors made for each example on the training or test set - so lower loss numbers are better. (In technical terms, for neural networks the loss is usually negative log-likelihood for classification, and residual sum of squares for regression.)

To show decision boundaries more clearly, along with the test data, click the two checkboxes marked "Show test data" and "Discretize output" in the lower right of the window.

To access the simulation, click here:

http://playground.tensorflow.org/



[READING: DEEP LEARNING IN A NUTSHELL]
This self-contained tutorial by Tim Dettmers covers the key high-level concepts of deep learning and reinforces the basic concepts we covered in the Neural Networks and Deep Learning lectures. There are multiple parts - Part 1 is less technical while Parts 2-4 go into more detail on algorithms.

The link to access Part 1 is here:

https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/

Deep Learning in a Nutshell: Core Concepts. (2016, September 08). Retrieved May 10, 2017.




[READING: ASSISTING PATHOLOGIST IN DETECTING CANCER WITH DEEP LEARNING]
This short article is an example of how deep learning is being used in healthcare.

Assisting Pathologists in Detecting Cancer with Deep Learning

Posted by Martin Stumpe (Technical Lead) and Lily Peng (Product Manager), Google Research Blog



[READING: THE TREACHERY OF LEAKAGE]
This fun, less-technical read from Colin Fraser reinforces the material in the Data Leakage lecture to provide further explanation and examples on detecting and avoiding data leakage in your machine learning applications.

Here's the link to the article:

https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931



[READING: LEAKAGE IN DATA MINING]
If you want an example in more depth of how data scientists are exploring ways to detect and avoid data leakage, this technical article proposes one approach: a two-stage process based on "legitimacy tags".

If you're just interested in getting a little more background on the problem along with interesting examples, Sections 1 and 2 (Introduction and Related Work) are also useful to read on their own.

Kaufman, S., Rosset, S., & Perlich, C. (2011). Leakage in data mining. Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11. doi:10.1145/2020408.2020496



[READING: DATA LEAKAGE EXAMPLE]
In 2013 a machine learning competition offered a prize for the most accurate detection of right whale calls based on audio data. The organizers soon discovered data leakage problems in the first release of the dataset, and this article explains what happened. It's a short but interesting article that serves as an excellent example of how subtle or not-so-subtle leakage can occur in specific features.

https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865#25839#post25839



[READING: RULES OF MACHINE LEARNING]
This optional reading is intended mainly for software engineers who want to build and deploy machine learning applications in production - especially at scale. The only background knowledge required are the basic machine learning concepts we've covered so far in this course. Written by Google's Dr. Martin Zinkevich, it walks through a set of software engineering best practices for designing and deploying machine learning in software systems - based on years of practical experience at Google.

http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf



[READING: How to Use t-SNE Effectively]
Wattenberg, et al., "How to Use t-SNE Effectively", Distill, 2016. http://doi.org/10.23915/distill.00002

http://distill.pub/2016/misread-tsne/#citation



[READING]
Gleesen, Peter. "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms", freeCodeCamp, 2017.

https://medium.freecodecamp.com/how-machines-make-sense-of-big-data-an-introduction-to-clustering-algorithms-4bd97d4fbaba


